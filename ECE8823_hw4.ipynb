{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE8823_hw4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlieLehman/convexoptimization/blob/master/ECE8823_hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CVT_iQVNj_mr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1."
      ]
    },
    {
      "metadata": {
        "id": "_L28BHAzj90J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2."
      ]
    },
    {
      "metadata": {
        "id": "JSeXgq5ijslV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.misc import derivative\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "        \n",
        "class GradientDescent(object):\n",
        "    SOLVER_PARAMS_DEFAULT = {\n",
        "        'backtracking':{\n",
        "            'alpha':0.001,\n",
        "            'beta':0.8\n",
        "        },\n",
        "        'newton':{\n",
        "            'alpha':0.001,\n",
        "            'beta':0.8\n",
        "        },\n",
        "        \n",
        "    }\n",
        "    def __init__(self, f, solver_params=SOLVER_PARAMS_DEFAULT):\n",
        "        self.f = f\n",
        "        self.solver_params = solver_params\n",
        "        self.alpha = 0.001\n",
        "        self.beta = 0.8\n",
        "        self.x = None\n",
        "        self.k = 0\n",
        "        self.grad_f = grad_f\n",
        "        \n",
        "    def solve(self, x0, max_iter, tol):\n",
        "    def _solve_newtwon(self, x0, max_iter, tol):\n",
        "    def _solve_backtracking(self, x0, max_iter, tol):\n",
        "        self.x = x0.copy()\n",
        "        for k in range(max_iter):\n",
        "            \n",
        "            # Update Direction\n",
        "            d = -self.grad_f(self.x).T \n",
        "            \n",
        "            # Termination Condition\n",
        "            _dd = d.T@d \n",
        "            if _dd <= tol:\n",
        "                self.k = k\n",
        "                return self.x\n",
        "            \n",
        "            # Update Step Size\n",
        "            t = self._backtracking(d, -_dd)\n",
        "            self.x += t*d\n",
        "        \n",
        "    def _backtracking(self, d, _dd):\n",
        "        t = 1\n",
        "        c1 = lambda t: self.f(self.x + t*d)\n",
        "        c2 = lambda t: self.f(self.x) + self.alpha*t*_dd\n",
        "        cond = c1(t) < c2(t)\n",
        "        while not cond:\n",
        "            t *= self.beta\n",
        "            cond = c1(t) < c2(t)\n",
        "        return t"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}